
********** MultinomialNB default values, try 1 **********
{'business': 510, 'entertainment': 386, 'politics': 417, 'sport': 511, 'tech': 401}
b) Confusion Matrix:
[[105   0   1   0   1]
 [  1  60   5   0   1]
 [  3   0  80   1   1]
 [  0   0   0  98   0]
 [  0   0   0   2  86]]

c) Classification Report: 
               precision    recall  f1-score   support

     business       0.96      0.98      0.97       107
entertainment       1.00      0.90      0.94        67
     politics       0.93      0.94      0.94        85
        sport       0.97      1.00      0.98        98
         tech       0.97      0.98      0.97        88

     accuracy                           0.96       445
    macro avg       0.97      0.96      0.96       445
 weighted avg       0.96      0.96      0.96       445

d) Accuracy:0.9640449438202248
   Macro F1:0.9618905324301682
   Weighted F1:0.9638286865028158
e) Prior probabilities:[-1.48543208 -1.71917754 -1.67923367 -1.46092105 -1.73816545]
f) Vocabulary Size:26770
g) Number of word-tokens in each class:[3514.172394448074, 2897.932400888115, 3253.8708610973363, 3567.327533295971, 3121.3774375545777]
h) Number of word-tokens in entire corpus:16354.680627284102
i) Number and percentage of words with zero frequency in each class:[16292, 16212, 16820, 17334, 16129]
j) Number and percentage of word with one frequency in entire corpus:0
k) Favorite words: Christmas and tree - [[-1.2667474  -1.17230942 -2.06391257 -2.16311678 -1.7916798 ]
 [-1.61549045 -1.33650199 -1.80065959 -1.59273309 -1.77263518]]

********** MultinomialNB default values, try 2 **********
{'business': 510, 'entertainment': 386, 'politics': 417, 'sport': 511, 'tech': 401}
b) Confusion Matrix:
[[105   0   1   0   1]
 [  1  60   5   0   1]
 [  3   0  80   1   1]
 [  0   0   0  98   0]
 [  0   0   0   2  86]]

c) Classification Report: 
               precision    recall  f1-score   support

     business       0.96      0.98      0.97       107
entertainment       1.00      0.90      0.94        67
     politics       0.93      0.94      0.94        85
        sport       0.97      1.00      0.98        98
         tech       0.97      0.98      0.97        88

     accuracy                           0.96       445
    macro avg       0.97      0.96      0.96       445
 weighted avg       0.96      0.96      0.96       445

d) Accuracy:0.9640449438202248
   Macro F1:0.9618905324301682
   Weighted F1:0.9638286865028158
e) Prior probabilities:[-1.48543208 -1.71917754 -1.67923367 -1.46092105 -1.73816545]
f) Vocabulary Size:26770
g) Number of word-tokens in each class:[3514.172394448074, 2897.932400888115, 3253.8708610973363, 3567.327533295971, 3121.3774375545777]
h) Number of word-tokens in entire corpus:16354.680627284102
i) Number and percentage of words with zero frequency in each class:[16292, 16212, 16820, 17334, 16129]
j) Number and percentage of word with one frequency in entire corpus:0
k) Favorite words: Christmas and tree - [[-1.2667474  -1.17230942 -2.06391257 -2.16311678 -1.7916798 ]
 [-1.61549045 -1.33650199 -1.80065959 -1.59273309 -1.77263518]]

********** MultinomialNB smoothing 0.0001 **********
{'business': 510, 'entertainment': 386, 'politics': 417, 'sport': 511, 'tech': 401}
b) Confusion Matrix:
[[100   0   5   0   2]
 [  1  63   1   0   2]
 [  4   0  80   0   1]
 [  0   0   0  98   0]
 [  0   0   0   0  88]]

c) Classification Report: 
               precision    recall  f1-score   support

     business       0.95      0.93      0.94       107
entertainment       1.00      0.94      0.97        67
     politics       0.93      0.94      0.94        85
        sport       1.00      1.00      1.00        98
         tech       0.95      1.00      0.97        88

     accuracy                           0.96       445
    macro avg       0.97      0.96      0.96       445
 weighted avg       0.96      0.96      0.96       445

d) Accuracy:0.9640449438202248
   Macro F1:0.9641350401746962
   Weighted F1:0.9640069264742638
e) Prior probabilities:[-1.48543208 -1.71917754 -1.67923367 -1.46092105 -1.73816545]
f) Vocabulary Size:26770
g) Number of word-tokens in each class:[3514.172394448074, 2897.932400888115, 3253.8708610973363, 3567.327533295971, 3121.3774375545777]
h) Number of word-tokens in entire corpus:16354.680627284102
i) Number and percentage of words with zero frequency in each class:[16292, 16212, 16820, 17334, 16129]
j) Number and percentage of word with one frequency in entire corpus:0
k) Favorite words: Christmas and tree - [[-1.22387298 -0.83172782 -2.5090655  -3.5680924  -1.82597405]
 [-8.83340777 -0.11677032 -8.95031152 -8.82389807 -2.20920578]]

********** MultinomialNB smoothing 0.9 **********
{'business': 510, 'entertainment': 386, 'politics': 417, 'sport': 511, 'tech': 401}
b) Confusion Matrix:
[[105   0   1   0   1]
 [  1  60   5   0   1]
 [  3   0  80   1   1]
 [  0   0   0  98   0]
 [  0   0   0   2  86]]

c) Classification Report: 
               precision    recall  f1-score   support

     business       0.96      0.98      0.97       107
entertainment       1.00      0.90      0.94        67
     politics       0.93      0.94      0.94        85
        sport       0.97      1.00      0.98        98
         tech       0.97      0.98      0.97        88

     accuracy                           0.96       445
    macro avg       0.97      0.96      0.96       445
 weighted avg       0.96      0.96      0.96       445

d) Accuracy:0.9640449438202248
   Macro F1:0.9618905324301682
   Weighted F1:0.9638286865028158
e) Prior probabilities:[-1.48543208 -1.71917754 -1.67923367 -1.46092105 -1.73816545]
f) Vocabulary Size:26770
g) Number of word-tokens in each class:[3514.172394448074, 2897.932400888115, 3253.8708610973363, 3567.327533295971, 3121.3774375545777]
h) Number of word-tokens in entire corpus:16354.680627284102
i) Number and percentage of words with zero frequency in each class:[16292, 16212, 16820, 17334, 16129]
j) Number and percentage of word with one frequency in entire corpus:0
k) Favorite words: Christmas and tree - [[-1.25883591 -1.15322291 -2.08492772 -2.20916102 -1.79402906]
 [-1.62911744 -1.30583003 -1.81344554 -1.60652997 -1.77622348]]